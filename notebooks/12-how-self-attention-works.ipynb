{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How and Why does Self Attention Work? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Non-Linear Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prevailing paradigm in machine learning is to repeatedly perform the same non-linear transformation, initially on the input data, and then on the outputs successively. Each transformation is called a layer. \"Deep\" architectures are so called because they have many layers -- for example, the GPT-3 model trained at OpenAI had 96 layers in total. \n",
    "\n",
    "Many of the key breakthroughs in recent years have focused on resolving the problems that crop up while training deep architectures. I have covered a few of these in previous posts, such as [Batch Normalization](...) and [Dropout](...).\n",
    "\n",
    "The earliest deep-learning architecture was what is now called a Feed-Forward Network, which in its current mature formulation, consists of a linear operation followed by a non-linear activation function $(\\sigma)$ such as ReLU (Rectified Linear Unit).\n",
    "\n",
    "$$\n",
    "f(\\bold{x}) = \\sigma(\\bold{Wx} + b)\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"nn.svg\" alt=\"Feed Forward Network\" style=\"width:500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular architecture used primarily in computer vision is the Convolutional Neural Network (CNN), whose basic transformation is the convolution followed by an activation function. The idea here is that the model can learn to detect features in an image by performing non-linear transformations on small patches of the image. Then, just like the basic Feed-Forward network, the same operation can be performed on the outputs successively.\n",
    "\n",
    "\n",
    "<img src=\"cnn.svg\" alt=\"Feed Forward Network\" style=\"width:800px;\"/>\n",
    "\n",
    "I won't go into to much detail on these here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A New Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of self-attention developed out of the sequence-to-sequence model architectures which were used primarily for machine translation. Here, the goal was to learn a single representation for an input sentence (\"encode\"), then use it to generate a translation (\"decode\").\n",
    "\n",
    "It however proved difficult to compress the information needed to translate a long sentence into a single representation. \n",
    "\n",
    "Bahadanau et. al. (2014) introduced the concept of attention -- the model could learn to use parts of the input sentence (\"attend\") directly while decoding rather than rely solely on the learnt representation. Parikh et al. (2016) realized that the attention operation itself could be used for NLP tasks such as entailment. Lin et al. (2017) introduced the concept of self-attention to perform a variety of NLP tasks. \n",
    "\n",
    "In my earlier post on [Nadaraya-Watson Regression](...), we saw how this classic non-parametric technique can be interpreted as an early form of attention. \n",
    "\n",
    "Vaswani el al. (2017) realized that the self-attention mechanism could be used as the basic non-linear transformation for sequences of variable length. This also allowed the model to process tokens in parallel by incorporating positional information (check out my post on positional embeddings [here](...)). By avoiding having to explicitly process each token in sequence, it became possible to train much deeper networks. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So What Exactly is Self Attention?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's See If it Works\n",
    "\n",
    "Let's train a sentiment classifier. For our dataset, we will be using Yelp review dataset with GloVE embeddings. Our baseline model is a simple feedforward network that uses an average of the word embeddings. The candidate model will use an additional self-attention layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikram/dev/xjax/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDb dataset from Hugging Face\n",
    "raw_train_data, raw_test_data = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .custom-paragraph {\n",
       "        width: 600px;\n",
       "        margin: auto;\n",
       "        line-height: 1.6;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "    .custom-paragraph {\n",
    "        width: 600px;\n",
    "        margin: auto;\n",
    "        line-height: 1.6;\n",
    "    }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"custom-paragraph\">\n",
       "              <p><b>[Negative]</b> I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.</p>\n",
       "             </div><div class=\"custom-paragraph\">\n",
       "              <p><b>[Negative]</b> \"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won't see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.</p>\n",
       "             </div><div class=\"custom-paragraph\">\n",
       "              <p><b>[Negative]</b> If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br /></p>\n",
       "             </div><div class=\"custom-paragraph\">\n",
       "              <p><b>[Negative]</b> This film was probably inspired by Godard's Masculin, f√©minin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.</p>\n",
       "             </div><div class=\"custom-paragraph\">\n",
       "              <p><b>[Negative]</b> Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />\"Is that all there is??\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \"Goodbye Columbus\"). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!<br /><br />The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.<br /><br />Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!<br /><br />Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren't for the censorship scandal, it would have been ignored, then forgotten.<br /><br />Instead, the \"I Am Blank, Blank\" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that \"naughty sex film\" that \"revolutionized the film industry\"...<br /><br />Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the \"dirty\" parts, just to get it over with.<br /><br /></p>\n",
       "             </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "n_samples = 5\n",
    "samples = list(islice(raw_train_data, n_samples))\n",
    "#samples = list(raw_train_data.take(n_samples).cache())\n",
    "html = \"\"\n",
    "for d in samples:\n",
    "    text = d['text']\n",
    "    label = \"Positive\" if d['label'] == 1 else \"Negative\"\n",
    "    html += f\"\"\"<div class=\"custom-paragraph\">\n",
    "              <p><b>[{label}]</b> {text}</p>\n",
    "             </div>\"\"\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing item 24999\r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def clean_paragraph(text):\n",
    "    # Keep only alphanumeric characters, punctuation, and spaces.\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<br\\s*/?>', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[-]', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[.]', '. ', text)\n",
    "    cleaned_text = re.sub(r'[^a-z0-9\\s\\.,!?;:-]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def process_raw_data(raw_data):\n",
    "    word_counts = Counter()\n",
    "    processed_data = []\n",
    "    for i, item in enumerate(raw_data):\n",
    "        print(f\"Processing item {i}\", end=\"\\r\")\n",
    "        # Split into sentences\n",
    "        text = item['text'] #.numpy().decode('utf-8')\n",
    "        text = clean_paragraph(text)\n",
    "        label = item['label']\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        word_counts.update(tokens)\n",
    "        processed_data.append((tokens, label))\n",
    "        # Update indices\n",
    "    sorted_counts = word_counts.most_common()\n",
    "    return sorted_counts, processed_data\n",
    "\n",
    "\n",
    "word_counts, processed_data = process_raw_data(raw_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 9998\n",
    "vocab = [ t[0] for t in word_counts[:VOCAB_SIZE] ]\n",
    "vocab += [ '<UNK>', '<PAD>' ]\n",
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "EMBED_DIM = 100\n",
    "\n",
    "def load_glove_embeddings(filepath):\n",
    "    embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Load the 100-dimensional GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings('../datasets/glove/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_embedding_matrix(vocab, glove_embeddings):\n",
    "    embedding_matrix = np.random.normal(size=(len(vocab), EMBED_DIM)).astype('float32')\n",
    "    for idx, word in enumerate(vocab):\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = build_embedding_matrix(vocab, glove_embeddings)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'Embed_0': {'embedding': Array([[-0.1462095 ,  0.06522508, -0.11902562, ..., -0.00575609,\n",
      "         0.04394421,  0.00789563],\n",
      "       [-0.1023385 , -0.07691558,  0.00242564, ...,  0.01657767,\n",
      "        -0.04966426,  0.05988198],\n",
      "       [ 0.18051139,  0.157578  , -0.02782032, ..., -0.06300503,\n",
      "        -0.20322858,  0.05086327],\n",
      "       ...,\n",
      "       [ 0.06365619,  0.08014771, -0.04117989, ...,  0.06356586,\n",
      "         0.0695641 ,  0.01247433],\n",
      "       [-0.13622792, -0.14932428, -0.04634263, ...,  0.05660709,\n",
      "        -0.05892929,  0.10624246],\n",
      "       [ 0.03298381, -0.10392094,  0.1450322 , ...,  0.11992919,\n",
      "        -0.08287467,  0.14703886]], dtype=float32)}, 'Dense_0': {'kernel': Array([[-0.09500722,  0.01477639],\n",
      "       [ 0.01441283,  0.19017147],\n",
      "       [-0.14262271,  0.02427947],\n",
      "       [-0.12324871, -0.03615518],\n",
      "       [ 0.10451844, -0.00641061],\n",
      "       [-0.09632497, -0.01472902],\n",
      "       [ 0.09709357, -0.0105184 ],\n",
      "       [-0.07130978, -0.01305214],\n",
      "       [-0.0410353 , -0.03165903],\n",
      "       [-0.01962214, -0.19930689],\n",
      "       [-0.1948133 , -0.10254476],\n",
      "       [-0.07013005, -0.00573085],\n",
      "       [-0.09185664,  0.04540499],\n",
      "       [ 0.0638411 , -0.01954777],\n",
      "       [-0.09959865,  0.07326508],\n",
      "       [-0.18585946, -0.10641588],\n",
      "       [-0.13995266, -0.07092629],\n",
      "       [-0.13977867,  0.15061395],\n",
      "       [-0.02856161,  0.03613282],\n",
      "       [ 0.03340371, -0.08831717],\n",
      "       [-0.03208793, -0.19316773],\n",
      "       [-0.07268266, -0.053834  ],\n",
      "       [ 0.05558369,  0.11453296],\n",
      "       [ 0.04188585, -0.028851  ],\n",
      "       [-0.11132841, -0.01089051],\n",
      "       [-0.05915202, -0.00267009],\n",
      "       [-0.07193998,  0.04001778],\n",
      "       [ 0.11059922, -0.02324469],\n",
      "       [ 0.16717708,  0.00566236],\n",
      "       [-0.15535493, -0.12034502],\n",
      "       [-0.0361856 , -0.07135957],\n",
      "       [ 0.19015397, -0.03341247],\n",
      "       [-0.05288954,  0.05292747],\n",
      "       [-0.19581723,  0.03256846],\n",
      "       [ 0.0946779 , -0.01525461],\n",
      "       [ 0.0615258 , -0.11396267],\n",
      "       [-0.01310593, -0.01872664],\n",
      "       [ 0.0113079 , -0.06385748],\n",
      "       [-0.05010979, -0.0196756 ],\n",
      "       [-0.1324016 ,  0.11290712],\n",
      "       [-0.11902224,  0.08902493],\n",
      "       [ 0.07358379, -0.01286166],\n",
      "       [-0.03604454, -0.05898497],\n",
      "       [-0.12697858, -0.12060896],\n",
      "       [-0.02209141, -0.09868776],\n",
      "       [-0.16248126, -0.07750048],\n",
      "       [ 0.00429477, -0.06700612],\n",
      "       [-0.01483254,  0.00815293],\n",
      "       [ 0.00484971, -0.05604417],\n",
      "       [ 0.17932089,  0.04814668],\n",
      "       [-0.1553773 ,  0.08518545],\n",
      "       [-0.06663015,  0.14954762],\n",
      "       [ 0.12017978,  0.09160983],\n",
      "       [ 0.05999082,  0.01434543],\n",
      "       [-0.13524102,  0.00951665],\n",
      "       [ 0.06412517,  0.0625671 ],\n",
      "       [ 0.12245851, -0.01742982],\n",
      "       [ 0.00640027,  0.11079256],\n",
      "       [-0.01386731, -0.00800695],\n",
      "       [-0.09430353,  0.06704156],\n",
      "       [-0.02471144,  0.11009154],\n",
      "       [ 0.05399005, -0.0550519 ],\n",
      "       [-0.09972446, -0.00745028],\n",
      "       [-0.10908297, -0.12813511],\n",
      "       [-0.09928943, -0.0661423 ],\n",
      "       [-0.12558249,  0.12843604],\n",
      "       [ 0.0956241 , -0.06515247],\n",
      "       [-0.08050641,  0.13688314],\n",
      "       [ 0.07681526, -0.06455029],\n",
      "       [ 0.00297589,  0.12642555],\n",
      "       [-0.09232663, -0.12906565],\n",
      "       [ 0.15292163, -0.03391438],\n",
      "       [-0.08712307,  0.01014166],\n",
      "       [-0.04029918, -0.00155961],\n",
      "       [ 0.0385876 ,  0.08452287],\n",
      "       [-0.00665347, -0.1910529 ],\n",
      "       [ 0.02649082, -0.08417181],\n",
      "       [ 0.08261531,  0.03370374],\n",
      "       [-0.1872034 ,  0.02495456],\n",
      "       [ 0.13099265, -0.05831274],\n",
      "       [ 0.1271964 ,  0.00589328],\n",
      "       [ 0.10639295,  0.04982699],\n",
      "       [ 0.02859052, -0.02500951],\n",
      "       [-0.0638658 , -0.12333785],\n",
      "       [-0.05174684,  0.09156212],\n",
      "       [-0.03796476, -0.04633104],\n",
      "       [ 0.07079358, -0.00728976],\n",
      "       [ 0.00408104,  0.06619566],\n",
      "       [ 0.12325588, -0.0766104 ],\n",
      "       [ 0.00613003,  0.00530742],\n",
      "       [ 0.0684298 , -0.02305098],\n",
      "       [-0.01175353,  0.06276494],\n",
      "       [ 0.08859618, -0.15040356],\n",
      "       [ 0.1779714 , -0.18416671],\n",
      "       [-0.09859411,  0.07714326],\n",
      "       [-0.19329825,  0.18295042],\n",
      "       [-0.02514856,  0.11268872],\n",
      "       [-0.02148466,  0.05166891],\n",
      "       [ 0.04627591, -0.02130597],\n",
      "       [ 0.04436265, -0.11626428],\n",
      "       [ 0.01479169,  0.02218312],\n",
      "       [ 0.02159061, -0.05496797],\n",
      "       [-0.09229118,  0.10390227],\n",
      "       [-0.09171864, -0.01510132],\n",
      "       [-0.08331016, -0.12859735],\n",
      "       [-0.18035719, -0.02923622],\n",
      "       [-0.00336924,  0.11506815],\n",
      "       [ 0.05595388,  0.04887412],\n",
      "       [ 0.12198092, -0.01350379],\n",
      "       [ 0.02597296, -0.00413055],\n",
      "       [-0.06408688,  0.00488934],\n",
      "       [ 0.0180921 , -0.03997947],\n",
      "       [ 0.08343626, -0.10141234],\n",
      "       [ 0.10847089, -0.12817764],\n",
      "       [ 0.03761346, -0.0198873 ],\n",
      "       [-0.06515855,  0.03038679],\n",
      "       [ 0.06053187, -0.08252704],\n",
      "       [-0.02364748,  0.02186342],\n",
      "       [-0.04161762, -0.02390526],\n",
      "       [-0.04250123,  0.10789962],\n",
      "       [ 0.10190307,  0.05477526],\n",
      "       [ 0.1531009 , -0.03718568],\n",
      "       [ 0.09040296,  0.04531274],\n",
      "       [-0.17867951,  0.05576309],\n",
      "       [ 0.08643256, -0.0015294 ],\n",
      "       [ 0.09160988,  0.03937492],\n",
      "       [ 0.11517324,  0.01627335],\n",
      "       [-0.19197452,  0.03305569]], dtype=float32), 'bias': Array([0., 0.], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "    num_classes: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embed the input tokens\n",
    "        embeddings = nn.Embed(num_embeddings=self.vocab_size, features=self.embed_dim)(x)\n",
    "\n",
    "        # Compute the mean of embeddings along the sequence dimension (global average pooling)\n",
    "        pooled = jnp.mean(embeddings, axis=1)\n",
    "\n",
    "        # A single dense layer for classification\n",
    "        logits = nn.Dense(self.num_classes)(pooled)\n",
    "        return logits\n",
    "\n",
    "# Initialize model and input data\n",
    "vocab_size = 5000   # Example: 5000 unique tokens\n",
    "embed_dim = 128      # Size of embedding vectors\n",
    "num_classes = 2      # Example: Binary classification (e.g., positive/negative sentiment)\n",
    "sequence_length = 32  # Input sequence length (e.g., 32 words)\n",
    "\n",
    "model = EmbeddingModel(vocab_size, embed_dim, num_classes)\n",
    "\n",
    "# Random input: Batch of 8 sequences with length 32 (word indices in range [0, vocab_size-1])\n",
    "rng = jax.random.PRNGKey(0)\n",
    "dummy_input = jax.random.randint(rng, (8, sequence_length), 0, vocab_size)\n",
    "\n",
    "# Initialize model parameters\n",
    "variables = model.init(rng, dummy_input)\n",
    "\n",
    "# Print the initialized parameters\n",
    "print(variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MyModel.__call__.<locals>.<lambda>() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocab), embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Since Flax models are functional, we need to initialize parameters\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(freeze(params), input_data)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)  \u001b[38;5;66;03m# Output shape: (2, 3, 100)\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36mMyModel.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m     embedding_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbed(\n\u001b[1;32m     12\u001b[0m         num_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m     13\u001b[0m         features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim,\n\u001b[1;32m     14\u001b[0m         embedding_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m shape, dtype: jnp\u001b[38;5;241m.\u001b[39marray(embedding_matrix)\n\u001b[1;32m     15\u001b[0m     )\n\u001b[0;32m---> 16\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[43membedding_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedded\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[0;32m~/dev/xjax/.venv/lib/python3.11/site-packages/flax/linen/linear.py:1106\u001b[0m, in \u001b[0;36mEmbed.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1106\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/dev/xjax/.venv/lib/python3.11/site-packages/flax/core/scope.py:990\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeCollectionNotFound(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamNotFoundError(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[0;32m--> 990\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mput_variable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, name, value)\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unbox:\n",
      "\u001b[0;31mTypeError\u001b[0m: MyModel.__call__.<locals>.<lambda>() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "# Example input batch (batch_size=2, sequence_length=3)\n",
    "input_data = jnp.array([[0, 1, 2], [1, 3, 4]])\n",
    "\n",
    "# Initialize and apply the model\n",
    "model = MyModel(vocab_size=len(vocab), embedding_dim=embedding_dim)\n",
    "\n",
    "# Since Flax models are functional, we need to initialize parameters\n",
    "params = model.init(jax.random.PRNGKey(42), input_data)\n",
    "output = model.apply(freeze(params), input_data)\n",
    "\n",
    "print(output)  # Output shape: (2, 3, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "list(glove_embeddings.keys())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1. Bahdanau et al. 2015\n",
    "2. Rocktashel et al. 2016 Reasoning about Entailment with Neural Attention ()\n",
    "3. Parikh, A Decomposable Attention Model for Natural Language Inference\n",
    "4. GloVe, Pennington et al. (2014)\n",
    "5. Lin et al. A Structured Self-Attentive Sentence Embedding\n",
    "6. GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
