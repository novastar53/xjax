{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "859faee9-290a-4117-8fe5-918a144087cf",
   "metadata": {},
   "source": [
    "# Skipgram Word Embeddings on the Simpsons Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42054f29-527d-44c1-85e2-372300967b40",
   "metadata": {},
   "source": [
    "## Load the Simpsons Dialogues Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceaed308-69df-4336-85e8-3498acac4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../datasets/simpsons_dataset.csv\")\n",
    "sentences = df[\"spoken_words\"].tolist()\n",
    "sentences = [ s for s in sentences if type(s) == str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4e6c8-3ba0-4a75-8c9a-a38afc265149",
   "metadata": {},
   "source": [
    "### Tokenize and Lemmatize Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7a2259-8d4b-4b53-ab7a-f6f8e5693f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Generate a set of stopwords to remove\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = stop_words.union(set([\"'s\", \"n't\", \"'m\", \"'re\", \"'ll\", \"'d\"]))\n",
    "punctuation = set(string.punctuation)\n",
    "stop_words = stop_words.union(punctuation)\n",
    "stop_words = stop_words.union(set([\"--\", \"..\", \"''\", \"...\", \"``\"]))\n",
    "digits = set([str(n) for n in range(10)])\n",
    "stop_words = stop_words.union(digits)\n",
    "\n",
    "# Map the NLTK/Treebank pos tags onto Wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if the tag is unknown\n",
    "\n",
    "# Generate cleaned sentences\n",
    "clean_sentences = []\n",
    "clean_tokens = []\n",
    "\n",
    "for s in sentences:\n",
    "  tokens = word_tokenize(s)\n",
    "  tokens = [t for t in tokens if t.lower() not in stop_words]\n",
    "  tagged_tokens = nltk.pos_tag(tokens)\n",
    "  tagged_tokens = [ (w, get_wordnet_pos(t)) for w,t in tagged_tokens ]\n",
    "  lemma_words = [lemmatizer.lemmatize(w.lower(), get_wordnet_pos(t)) for w,t in tagged_tokens]\n",
    "  clean_sentences.append(lemma_words)\n",
    "  clean_tokens += lemma_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b28032f-2ad5-4790-910a-2d179106eb9d",
   "metadata": {},
   "source": [
    "### Generate the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f60732-3481-4065-86e6-5abb126e4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "VOCAB_SIZE=6000\n",
    "\n",
    "counts = Counter(clean_tokens).items()\n",
    "sorted_counts = sorted(counts, key=lambda k: k[1], reverse=True)\n",
    "vocab = sorted_counts[:VOCAB_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3146f8fc-3208-472b-a3e6-ecc40e29b6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oh', 8125),\n",
       " ('like', 6131),\n",
       " ('well', 6037),\n",
       " ('get', 5511),\n",
       " ('one', 4947),\n",
       " ('know', 4914),\n",
       " (\"'ve\", 4664),\n",
       " ('got', 4612),\n",
       " ('hey', 4277),\n",
       " ('homer', 4232)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8edf9d2c-0d58-4b17-ac79-7d758e4e14ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_tok = dict()\n",
    "tok_to_idx = dict()\n",
    "for idx,(tok, count) in enumerate(vocab):\n",
    "  idx_to_tok[idx] = tok\n",
    "  tok_to_idx[tok] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c4716-c36b-4b65-bb3a-e020308932e9",
   "metadata": {},
   "source": [
    "### Generate Positive Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb55b80-635a-447e-b276-504214db3bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1881262, 2)\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "WINDOW_SIZE=6\n",
    "\n",
    "len_buffer = WINDOW_SIZE//2\n",
    "\n",
    "\n",
    "def gen_dataset():\n",
    "  dataset = []\n",
    "  for s in clean_sentences:\n",
    "    for i in range(len(s)):\n",
    "      for j in range(max(0,i-len_buffer), min(len(s),i+len_buffer)):\n",
    "        if i != j:\n",
    "            if s[i] in tok_to_idx and s[j] in tok_to_idx:\n",
    "              idx_i = tok_to_idx[s[i]]\n",
    "              idx_j = tok_to_idx[s[j]]\n",
    "              dataset.append((idx_i,idx_j))\n",
    "  return dataset\n",
    "\n",
    "\n",
    "dataset = jnp.array(gen_dataset())\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f512945-7086-46af-a85a-f6bc347df54d",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c027406d-ff73-443e-aa09-85c321c9ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import jax\n",
    "import xjax\n",
    "from xjax.signals import train_epoch_completed\n",
    "\n",
    "# Module logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Generate a random seed\n",
    "seed = 42\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "\n",
    "# Model Params\n",
    "EMBEDDING_SIZE=100\n",
    "K=50\n",
    "NEG_PER_POS = 20\n",
    "\n",
    "# Train Params\n",
    "BATCH_SIZE = 100000\n",
    "NUM_ITER = len(dataset)//BATCH_SIZE\n",
    "NUM_EPOCHS = 50\n",
    "LR = 0.03\n",
    "\n",
    "# Create the model and initialize params\n",
    "model, params = xjax.models.sgns.sgns(rng=rng, vocab_size=VOCAB_SIZE, \n",
    "                             embedding_size=EMBEDDING_SIZE)\n",
    "\n",
    "# Train\n",
    "params = xjax.models.sgns.train(model, rng=rng, params=params,\n",
    "                               X=dataset,\n",
    "                               neg_per_pos=NEG_PER_POS,\n",
    "                               K=K,\n",
    "                               epochs=NUM_EPOCHS,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               learning_rate=LR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca4fa3-f783-42ca-83f5-5399d5ee8eeb",
   "metadata": {},
   "source": [
    "### Inspect the trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "596caea2-64c8-4434-b54a-3217ecbe61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(params, word, n=10):\n",
    "    if word not in tok_to_idx:\n",
    "        raise ValueError(f\"Word '{word}' not found in word vectors dictionary.\")\n",
    "\n",
    "    # Get the vector for the selected word\n",
    "    emb1 = params[tok_to_idx[word], :, 0]\n",
    "\n",
    "    # Calculate cosine similarities with all other words\n",
    "    similarities = {}\n",
    "    for other_word, other_idx in tok_to_idx.items():\n",
    "        if other_word != word:\n",
    "            emb2 = params[other_idx, :, 0]\n",
    "            similarity = jnp.dot(emb1,emb2)/(jnp.linalg.norm(emb1)*jnp.linalg.norm(emb2))\n",
    "            similarities[other_word] = similarity\n",
    "\n",
    "    # Sort by similarity and return the top n words\n",
    "    most_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    return most_similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3591e296-1b8a-4f02-b64f-bda241950ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elementary', Array(0.7011733, dtype=float32)),\n",
       " ('town', Array(0.6202735, dtype=float32)),\n",
       " ('city', Array(0.61156374, dtype=float32)),\n",
       " ('school', Array(0.5965495, dtype=float32)),\n",
       " ('today', Array(0.59161586, dtype=float32)),\n",
       " ('first', Array(0.5809513, dtype=float32)),\n",
       " ('country', Array(0.5762146, dtype=float32)),\n",
       " ('simpson', Array(0.55961263, dtype=float32)),\n",
       " ('history', Array(0.5421661, dtype=float32)),\n",
       " ('world', Array(0.5390263, dtype=float32))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar(params, \"springfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d74153-7f2b-40b1-bdcb-44755a77ed4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
