{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceaed308-69df-4336-85e8-3498acac4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"simpsons_dataset.csv\")\n",
    "sentences = df[\"spoken_words\"].tolist()\n",
    "sentences = [ s for s in sentences if type(s) == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7a2259-8d4b-4b53-ab7a-f6f8e5693f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vikram/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vikram/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/vikram/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vikram/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/vikram/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = stop_words.union(set([\"'s\", \"n't\", \"'m\", \"'re\", \"'ll\", \"'d\"]))\n",
    "punctuation = set(string.punctuation)\n",
    "stop_words = stop_words.union(punctuation)\n",
    "stop_words = stop_words.union(set([\"--\", \"..\", \"''\", \"...\", \"``\"]))\n",
    "digits = set([str(n) for n in range(10)])\n",
    "stop_words = stop_words.union(digits)\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if the tag is unknown\n",
    "\n",
    "clean_sentences = []\n",
    "clean_tokens = []\n",
    "\n",
    "for s in sentences:\n",
    "  tokens = word_tokenize(s)\n",
    "  tokens = [t for t in tokens if t.lower() not in stop_words]\n",
    "  tagged_tokens = nltk.pos_tag(tokens)\n",
    "  tagged_tokens = [ (w, get_wordnet_pos(t)) for w,t in tagged_tokens ]\n",
    "  lemma_words = [lemmatizer.lemmatize(w.lower(), get_wordnet_pos(t)) for w,t in tagged_tokens]\n",
    "  clean_sentences.append(lemma_words)\n",
    "  clean_tokens += lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f60732-3481-4065-86e6-5abb126e4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "VOCAB_SIZE=6000\n",
    "\n",
    "counts = Counter(clean_tokens).items()\n",
    "sorted_counts = sorted(counts, key=lambda k: k[1], reverse=True)\n",
    "vocab = sorted_counts[:VOCAB_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf9d2c-0d58-4b17-ac79-7d758e4e14ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_tok = dict()\n",
    "tok_to_idx = dict()\n",
    "for idx,(tok, count) in enumerate(vocab):\n",
    "  idx_to_tok[idx] = tok\n",
    "  tok_to_idx[tok] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb55b80-635a-447e-b276-504214db3bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE=6\n",
    "\n",
    "len_buffer = WINDOW_SIZE//2\n",
    "\n",
    "\n",
    "def gen_dataset():\n",
    "  dataset = []\n",
    "  for s in clean_sentences:\n",
    "    for i in range(len(s)):\n",
    "      for j in range(max(0,i-len_buffer), min(len(s),i+len_buffer)):\n",
    "        if i != j:\n",
    "            if s[i] in tok_to_idx and s[j] in tok_to_idx:\n",
    "              idx_i = tok_to_idx[s[i]]\n",
    "              idx_j = tok_to_idx[s[j]]\n",
    "              dataset.append((idx_i,idx_j))\n",
    "  return dataset\n",
    "\n",
    "\n",
    "dataset = jnp.array(gen_dataset())\n",
    "print(dataset.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
