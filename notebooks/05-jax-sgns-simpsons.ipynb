{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "859faee9-290a-4117-8fe5-918a144087cf",
   "metadata": {},
   "source": [
    "# Skipgram Word Embeddings on the Simpsons Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42054f29-527d-44c1-85e2-372300967b40",
   "metadata": {},
   "source": [
    "## Load the Simpsons Dialogues Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceaed308-69df-4336-85e8-3498acac4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../datasets/simpsons_dataset.csv\")\n",
    "sentences = df[\"spoken_words\"].tolist()\n",
    "sentences = [ s for s in sentences if type(s) == str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4e6c8-3ba0-4a75-8c9a-a38afc265149",
   "metadata": {},
   "source": [
    "### Tokenize and Lemmatize Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7a2259-8d4b-4b53-ab7a-f6f8e5693f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Generate a set of stopwords to remove\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = stop_words.union(set([\"'s\", \"n't\", \"'m\", \"'re\", \"'ll\", \"'d\"]))\n",
    "punctuation = set(string.punctuation)\n",
    "stop_words = stop_words.union(punctuation)\n",
    "stop_words = stop_words.union(set([\"--\", \"..\", \"''\", \"...\", \"``\"]))\n",
    "digits = set([str(n) for n in range(10)])\n",
    "stop_words = stop_words.union(digits)\n",
    "\n",
    "# Map the NLTK/Treebank pos tags onto Wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if the tag is unknown\n",
    "\n",
    "# Generate cleaned sentences\n",
    "clean_sentences = []\n",
    "clean_tokens = []\n",
    "\n",
    "for s in sentences:\n",
    "  tokens = word_tokenize(s)\n",
    "  tokens = [t for t in tokens if t.lower() not in stop_words]\n",
    "  tagged_tokens = nltk.pos_tag(tokens)\n",
    "  tagged_tokens = [ (w, get_wordnet_pos(t)) for w,t in tagged_tokens ]\n",
    "  lemma_words = [lemmatizer.lemmatize(w.lower(), get_wordnet_pos(t)) for w,t in tagged_tokens]\n",
    "  clean_sentences.append(lemma_words)\n",
    "  clean_tokens += lemma_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b28032f-2ad5-4790-910a-2d179106eb9d",
   "metadata": {},
   "source": [
    "### Generate the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f60732-3481-4065-86e6-5abb126e4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "VOCAB_SIZE=6000\n",
    "\n",
    "counts = Counter(clean_tokens).items()\n",
    "sorted_counts = sorted(counts, key=lambda k: k[1], reverse=True)\n",
    "vocab = sorted_counts[:VOCAB_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3146f8fc-3208-472b-a3e6-ecc40e29b6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oh', 8125),\n",
       " ('like', 6131),\n",
       " ('well', 6037),\n",
       " ('get', 5511),\n",
       " ('one', 4947),\n",
       " ('know', 4914),\n",
       " (\"'ve\", 4664),\n",
       " ('got', 4612),\n",
       " ('hey', 4277),\n",
       " ('homer', 4232)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8edf9d2c-0d58-4b17-ac79-7d758e4e14ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_tok = dict()\n",
    "tok_to_idx = dict()\n",
    "for idx,(tok, count) in enumerate(vocab):\n",
    "  idx_to_tok[idx] = tok\n",
    "  tok_to_idx[tok] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c4716-c36b-4b65-bb3a-e020308932e9",
   "metadata": {},
   "source": [
    "### Generate Positive Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb55b80-635a-447e-b276-504214db3bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1881262, 2)\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "WINDOW_SIZE=6\n",
    "\n",
    "len_buffer = WINDOW_SIZE//2\n",
    "\n",
    "\n",
    "def gen_dataset():\n",
    "  dataset = []\n",
    "  for s in clean_sentences:\n",
    "    for i in range(len(s)):\n",
    "      for j in range(max(0,i-len_buffer), min(len(s),i+len_buffer)):\n",
    "        if i != j:\n",
    "            if s[i] in tok_to_idx and s[j] in tok_to_idx:\n",
    "              idx_i = tok_to_idx[s[i]]\n",
    "              idx_j = tok_to_idx[s[j]]\n",
    "              dataset.append((idx_i,idx_j))\n",
    "  return dataset\n",
    "\n",
    "\n",
    "dataset = jnp.array(gen_dataset())\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f512945-7086-46af-a85a-f6bc347df54d",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c027406d-ff73-443e-aa09-85c321c9ea40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch=0, loss=92.3090, elapsed=7.0175\n",
      "INFO:__main__:epoch=1, loss=6.4165, elapsed=13.6926\n",
      "INFO:__main__:epoch=2, loss=5.2345, elapsed=20.4883\n",
      "INFO:__main__:epoch=3, loss=4.2222, elapsed=27.1027\n",
      "INFO:__main__:epoch=4, loss=3.7006, elapsed=33.8759\n",
      "INFO:__main__:epoch=5, loss=3.3958, elapsed=40.5573\n",
      "INFO:__main__:epoch=6, loss=3.1780, elapsed=47.8143\n",
      "INFO:__main__:epoch=7, loss=3.0147, elapsed=54.4782\n",
      "INFO:__main__:epoch=8, loss=2.9175, elapsed=61.1838\n",
      "INFO:__main__:epoch=9, loss=2.8482, elapsed=67.8347\n",
      "INFO:__main__:epoch=10, loss=2.7969, elapsed=74.5253\n",
      "INFO:__main__:epoch=11, loss=2.7111, elapsed=81.1747\n",
      "INFO:__main__:epoch=12, loss=2.6632, elapsed=87.9641\n",
      "INFO:__main__:epoch=13, loss=2.6241, elapsed=94.5831\n",
      "INFO:__main__:epoch=14, loss=2.5984, elapsed=101.1758\n",
      "INFO:__main__:epoch=15, loss=2.5634, elapsed=107.8927\n",
      "INFO:__main__:epoch=16, loss=2.5412, elapsed=114.5396\n",
      "INFO:__main__:epoch=17, loss=2.5138, elapsed=121.3133\n",
      "INFO:__main__:epoch=18, loss=2.4846, elapsed=127.9810\n",
      "INFO:__main__:epoch=19, loss=2.4693, elapsed=134.7078\n",
      "INFO:__main__:epoch=20, loss=2.4702, elapsed=141.4526\n",
      "INFO:__main__:epoch=21, loss=2.4458, elapsed=148.1216\n",
      "INFO:__main__:epoch=22, loss=2.4268, elapsed=154.7719\n",
      "INFO:__main__:epoch=23, loss=2.4111, elapsed=161.4819\n",
      "INFO:__main__:epoch=24, loss=2.3915, elapsed=168.1434\n",
      "INFO:__main__:epoch=25, loss=2.3803, elapsed=174.8375\n",
      "INFO:__main__:epoch=26, loss=2.3712, elapsed=181.5145\n",
      "INFO:__main__:epoch=27, loss=2.3433, elapsed=188.2208\n",
      "INFO:__main__:epoch=28, loss=2.3533, elapsed=194.8236\n",
      "INFO:__main__:epoch=29, loss=2.3292, elapsed=201.4996\n",
      "INFO:__main__:epoch=30, loss=2.3255, elapsed=208.1872\n",
      "INFO:__main__:epoch=31, loss=2.3142, elapsed=214.8694\n",
      "INFO:__main__:epoch=32, loss=2.2925, elapsed=221.4869\n",
      "INFO:__main__:epoch=33, loss=2.2855, elapsed=228.2925\n",
      "INFO:__main__:epoch=34, loss=2.2832, elapsed=235.1204\n",
      "INFO:__main__:epoch=35, loss=2.2780, elapsed=241.8601\n",
      "INFO:__main__:epoch=36, loss=2.2619, elapsed=248.5345\n",
      "INFO:__main__:epoch=37, loss=2.2655, elapsed=255.3013\n",
      "INFO:__main__:epoch=38, loss=2.2435, elapsed=261.9937\n",
      "INFO:__main__:epoch=39, loss=2.2515, elapsed=268.6537\n",
      "INFO:__main__:epoch=40, loss=2.2482, elapsed=275.4399\n",
      "INFO:__main__:epoch=41, loss=2.2305, elapsed=282.0826\n",
      "INFO:__main__:epoch=42, loss=2.2254, elapsed=288.8734\n",
      "INFO:__main__:epoch=43, loss=2.2110, elapsed=295.6136\n",
      "INFO:__main__:epoch=44, loss=2.2029, elapsed=302.2775\n",
      "INFO:__main__:epoch=45, loss=2.1982, elapsed=308.9334\n",
      "INFO:__main__:epoch=46, loss=2.1988, elapsed=315.6585\n",
      "INFO:__main__:epoch=47, loss=2.1888, elapsed=322.3229\n",
      "INFO:__main__:epoch=48, loss=2.1778, elapsed=329.0233\n",
      "INFO:__main__:epoch=49, loss=2.1799, elapsed=335.8148\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import jax\n",
    "import xjax\n",
    "from xjax.signals import train_epoch_completed\n",
    "\n",
    "# Module logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Generate a random seed\n",
    "seed = 42\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "\n",
    "# Model Params\n",
    "EMBEDDING_SIZE=100\n",
    "K=50\n",
    "NEG_PER_POS = 20\n",
    "\n",
    "# Train Params\n",
    "BATCH_SIZE = 100000\n",
    "NUM_ITER = len(dataset)//BATCH_SIZE\n",
    "NUM_EPOCHS = 50\n",
    "LR = 0.03\n",
    "\n",
    "# Create the model and initialize params\n",
    "model, params = xjax.models.sgns.sgns(rng=rng, vocab_size=VOCAB_SIZE, \n",
    "                             embedding_size=EMBEDDING_SIZE)\n",
    "\n",
    "\n",
    "#Log events\n",
    "@train_epoch_completed.connect_via(model)\n",
    "def collect_events(_, *, epoch, loss, elapsed, **__):\n",
    "    logger.info(f\"epoch={epoch}, loss={loss:0.4f}, elapsed={elapsed:0.4f}\")\n",
    "\n",
    "# Train\n",
    "params = xjax.models.sgns.train(model, rng=rng, params=params,\n",
    "                               X=dataset,\n",
    "                               neg_per_pos=NEG_PER_POS,\n",
    "                               K=K,\n",
    "                               epochs=NUM_EPOCHS,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               learning_rate=LR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca4fa3-f783-42ca-83f5-5399d5ee8eeb",
   "metadata": {},
   "source": [
    "### Inspect the trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "596caea2-64c8-4434-b54a-3217ecbe61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(params, word, n=10):\n",
    "    if word not in tok_to_idx:\n",
    "        raise ValueError(f\"Word '{word}' not found in word vectors dictionary.\")\n",
    "\n",
    "    # Get the vector for the selected word\n",
    "    emb1 = params[tok_to_idx[word], :, 0]\n",
    "\n",
    "    # Calculate cosine similarities with all other words\n",
    "    similarities = {}\n",
    "    for other_word, other_idx in tok_to_idx.items():\n",
    "        if other_word != word:\n",
    "            emb2 = params[other_idx, :, 0]\n",
    "            similarity = jnp.dot(emb1,emb2)/(jnp.linalg.norm(emb1)*jnp.linalg.norm(emb2))\n",
    "            similarities[other_word] = similarity\n",
    "\n",
    "    # Sort by similarity and return the top n words\n",
    "    most_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    return most_similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3591e296-1b8a-4f02-b64f-bda241950ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elementary', Array(0.7011735, dtype=float32)),\n",
       " ('town', Array(0.62027335, dtype=float32)),\n",
       " ('city', Array(0.61156374, dtype=float32)),\n",
       " ('school', Array(0.5965495, dtype=float32)),\n",
       " ('today', Array(0.59161603, dtype=float32)),\n",
       " ('first', Array(0.5809512, dtype=float32)),\n",
       " ('country', Array(0.5762146, dtype=float32)),\n",
       " ('simpson', Array(0.55961263, dtype=float32)),\n",
       " ('history', Array(0.5421662, dtype=float32)),\n",
       " ('world', Array(0.53902626, dtype=float32))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar(params, \"springfield\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
