{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Basic Character RNN Trained to Generate Novel Dinosaur Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by reading the dinosaur names dataset and extracting all the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = open(\"../datasets/dinos.txt\", \"r\").read()\n",
    "lines = text.split(\"\\n\")\n",
    "lines = [ l.strip().lower() for l in lines ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks like we have 1542 dinosaur names in the dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"Looks like we have {len(lines)} dinosaur names in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at a few dinosaur names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aachenosaurus',\n",
       " 'aardonyx',\n",
       " 'abdallahsaurus',\n",
       " 'abelisaurus',\n",
       " 'abrictosaurus',\n",
       " 'abrosaurus',\n",
       " 'abydosaurus',\n",
       " 'acanthopholis',\n",
       " 'achelousaurus',\n",
       " 'acheroraptor']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long are the dinosaur names? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aachenosaurus\n",
      "abdallahsaurus\n",
      "acrocanthosaurus\n",
      "archaeodontosaurus\n",
      "carcharodontosaurus\n",
      "micropachycephalosaurus\n",
      "The longest name is micropachycephalosaurus which is 23 characters.\n",
      "The shortest name is mei which is 3 characters.\n"
     ]
    }
   ],
   "source": [
    "lengths = [ len(name) for name in lines ]\n",
    "max_len = 0\n",
    "longest_name = None\n",
    "min_len = float('inf')\n",
    "shortest_name = None\n",
    "\n",
    "for line in lines:\n",
    "    l = len(line)\n",
    "    if l > max_len:\n",
    "        max_len = l\n",
    "        longest_name = line\n",
    "        print(longest_name)\n",
    "    if l < min_len:\n",
    "        min_len = l \n",
    "        shortest_name = line\n",
    "\n",
    "print(f\"The longest name is {longest_name} which is {max_len} characters.\")\n",
    "print(f\"The shortest name is {shortest_name} which is {min_len} characters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we'll be training a character RNN, let's tokenize the data by character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate a vocabulary\n",
    "counts = {}\n",
    "for line in lines:\n",
    "    chars = list(line)\n",
    "    for c in chars:\n",
    "        if c not in counts:\n",
    "            counts[c] = 1\n",
    "        else:\n",
    "            counts[c] += 1\n",
    "\n",
    "vocab = sorted(list(counts.keys()))\n",
    "\n",
    "VOCAB_SIZE = len(vocab) + 1\n",
    "\n",
    "# Then create token lookup indices\n",
    "tok_to_idx = {}\n",
    "idx_to_tok = {}\n",
    "\n",
    "for i,c in enumerate(vocab):\n",
    "    tok_to_idx[c] = i\n",
    "    idx_to_tok[i] = c\n",
    "\n",
    "\n",
    "# Add a stop character to the token index lookup\n",
    "tok_to_idx[\"@\"] = VOCAB_SIZE - 1\n",
    "idx_to_tok[VOCAB_SIZE - 1] = \"@\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's generate the training data. \n",
    "Since this is a sequence to sequence model, it has to learn to predict the next token in the sequence. Each training example therefore consists of an input sequence of tokens. The label is just the same sequence shifted to the right by a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "#Generate the dataset \n",
    "X = []\n",
    "Y = []\n",
    "for line in lines:\n",
    "    tokens = [ tok_to_idx[c] for c in line ]\n",
    "    tokens += [VOCAB_SIZE]*(max_len - len(tokens))\n",
    "    X.append(jnp.array([VOCAB_SIZE] + tokens))\n",
    "    Y.append(jnp.array(tokens + [VOCAB_SIZE]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, let's train the model. \n",
    "\n",
    "This is a simple RNN model. Its key feature is a vector $ h_t $, called the \"hidden state\", which remembers the context from the previous tokens in the input sequence. Observe that the output $ y_t $ is a function of the previous hidden state $ h_{t-1} $. This is what enables it to learn to predict the next token in a sequence. \n",
    "\n",
    "The input $ x_t $ is a one-hot vector that encodes the input token. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{align} h_t = tanh((W_{hx}x_{t} + W_{hh}h_{t-1}) + b_h) \\\\\n",
    "y_t = softmax(W_{yh}h_t + b_y)  \\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be interpreted as a two layer neural network. The input to the network can now be seen as a single vector consisting of the current token $ x_t $ and the hidden state $ h_{t-1} $, representing the previous tokens in the sequence already seen by the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{align} h_t = tanh( \\begin{bmatrix} W_{hx} W_{hh} \\end{bmatrix} \\begin{bmatrix} x_t \\\\ h_{t-1} \\end{bmatrix} + b_h) \\\\ \n",
    "y_t = softmax(W_{yh}h_t + b_y)  \\end{align} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Char RNN:epoch=0, loss=5101.4146, elapsed=1.7701\n",
      "INFO:Char RNN:epoch=10, loss=2159.6357, elapsed=11.1020\n",
      "INFO:Char RNN:epoch=20, loss=1812.7123, elapsed=20.4835\n",
      "INFO:Char RNN:epoch=30, loss=1953.0304, elapsed=29.7877\n",
      "INFO:Char RNN:epoch=40, loss=1881.3508, elapsed=39.0794\n",
      "INFO:Char RNN:epoch=50, loss=1878.3793, elapsed=48.3536\n",
      "INFO:Char RNN:epoch=60, loss=1782.6591, elapsed=57.6883\n",
      "INFO:Char RNN:epoch=70, loss=1772.7905, elapsed=67.0791\n",
      "INFO:Char RNN:epoch=80, loss=1757.2007, elapsed=76.3859\n",
      "INFO:Char RNN:epoch=90, loss=1774.6650, elapsed=86.0161\n",
      "INFO:Char RNN:epoch=100, loss=1717.6869, elapsed=95.5716\n",
      "INFO:Char RNN:epoch=110, loss=1778.9965, elapsed=105.1843\n",
      "INFO:Char RNN:epoch=120, loss=1684.2278, elapsed=115.7771\n",
      "INFO:Char RNN:epoch=130, loss=1680.6908, elapsed=125.9145\n",
      "INFO:Char RNN:epoch=140, loss=1701.4093, elapsed=135.4562\n",
      "INFO:Char RNN:epoch=150, loss=1704.2050, elapsed=147.0271\n",
      "INFO:Char RNN:epoch=160, loss=1710.7242, elapsed=157.2308\n",
      "INFO:Char RNN:epoch=170, loss=1703.7645, elapsed=167.0825\n",
      "INFO:Char RNN:epoch=180, loss=1696.4875, elapsed=177.4268\n",
      "INFO:Char RNN:epoch=190, loss=1680.0598, elapsed=187.4623\n",
      "INFO:Char RNN:epoch=200, loss=1706.4365, elapsed=198.3394\n",
      "INFO:Char RNN:epoch=210, loss=1681.7545, elapsed=209.2920\n",
      "INFO:Char RNN:epoch=220, loss=1711.7104, elapsed=219.7032\n",
      "INFO:Char RNN:epoch=230, loss=1675.1003, elapsed=230.2321\n",
      "INFO:Char RNN:epoch=240, loss=1653.1359, elapsed=240.7544\n",
      "INFO:Char RNN:epoch=250, loss=1754.8462, elapsed=250.7033\n",
      "INFO:Char RNN:epoch=260, loss=1688.5555, elapsed=260.5469\n",
      "INFO:Char RNN:epoch=270, loss=1648.2821, elapsed=269.9873\n",
      "INFO:Char RNN:epoch=280, loss=1656.9261, elapsed=279.7185\n",
      "INFO:Char RNN:epoch=290, loss=1687.1157, elapsed=289.7496\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import jax\n",
    "import xjax\n",
    "from xjax.signals import train_epoch_completed\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Module logger\n",
    "logger = logging.getLogger(\"Char RNN\")\n",
    "\n",
    "# Define hyperparameters\n",
    "HIDDEN_SIZE = 64\n",
    "VOCAB_SIZE = len(tok_to_idx)\n",
    "learning_rate = 1E-2\n",
    "max_grad = 10\n",
    "epochs = 300\n",
    "\n",
    "rng = jax.random.key(42)\n",
    "\n",
    "# I define a character-rnn model\n",
    "params, model = xjax.models.char_rnn.char_rnn(rng, VOCAB_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "# I log events\n",
    "@train_epoch_completed.connect_via(model)\n",
    "def collect_events(_, *, epoch, loss, elapsed, **__):\n",
    "    logger.info(f\"epoch={epoch}, loss={loss:0.4f}, elapsed={elapsed:0.4f}\")\n",
    "\n",
    "# I train a character RNN model on the data \n",
    "trained_params = xjax.models.char_rnn.train(model, rng=rng, params=params, \n",
    "                                            X_train=X, Y_train=Y, \n",
    "                                            vocab_size=VOCAB_SIZE, \n",
    "                                            epochs=epochs, \n",
    "                                            learning_rate=learning_rate,\n",
    "                                            max_grad=max_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's look at what the model learned\n",
    "Ideally, the model should have learned to generate plausible-sounding dinosaur names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_str = \"a\"\n",
    "prefix = [tok_to_idx[c] for c in prefix_str]\n",
    "generated = []\n",
    "for i in range(30):\n",
    "    rng, sub_rng = jax.random.split(rng)\n",
    "    y = xjax.models.char_rnn.generate(rng=sub_rng, prefix= prefix, params=trained_params, hidden_size=HIDDEN_SIZE, vocab_size=VOCAB_SIZE) \n",
    "    generated.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agraton\n",
      "agrososaurus\n",
      "agryinta\n",
      "aucastosaurus\n",
      "agrasstoiltesaurus\n",
      "aucanopus\n",
      "agrasoctomus\n",
      "austosus\n",
      "agroryonskosaurus\n",
      "aucastoreritochengon\n",
      "agrapikus\n",
      "aucanatopus\n",
      "auchttimti\n",
      "anotosaurus\n",
      "ageacingorosaurus\n",
      "aspatarosaurus\n",
      "aniloctor\n",
      "anintrantosaurucan\n",
      "agertoncttoptingonguandyn\n",
      "agramosaurus\n",
      "aucrenosaurusaurus\n",
      "agrandosaurus\n",
      "agrangosaurus\n",
      "agrontochuos\n",
      "auclachrasaurus\n",
      "abroplos\n",
      "agrodan\n",
      "agronongosaurus\n",
      "aucactodorodox\n",
      "adrangynotron\n"
     ]
    }
   ],
   "source": [
    "for g in generated:\n",
    "    print(\"\".join([idx_to_tok[i] for i in g[:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these look plausible. It seems to have figured out that dinosaur names tend to end with an 'rus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
