{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Performance of Gated Recurrent Units With and Without Attention\n",
    "...by learning to predict the text of H.G. Wells' The Time Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = open('../datasets/timemachine.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's tokenize the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikram/dev/xjax/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Remove punctuation and convert to lowercase\n",
    "text = raw_text.lower()\n",
    "text = re.sub(r'[^a-z]+', ' ', text)\n",
    "\n",
    "# Tokenize into characters\n",
    "char_tokenized_text = list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 27\n",
      "  35850\n",
      "e 19667\n",
      "t 15040\n",
      "a 12700\n",
      "i 11254\n",
      "o 11082\n",
      "n 10943\n",
      "s 9242\n",
      "r 8833\n",
      "h 8786\n"
     ]
    }
   ],
   "source": [
    "# Most common tokens\n",
    "from collections import Counter\n",
    "# Flatten the list of tokens\n",
    "# Count the tokens\n",
    "token_counts = Counter(char_tokenized_text)\n",
    "print(f\"Total tokens: {len(token_counts)}\")\n",
    "# Most common tokens\n",
    "most_common_tokens = token_counts.most_common(10)\n",
    "for token, count in most_common_tokens:\n",
    "    print(f\"{token} {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 28\n"
     ]
    }
   ],
   "source": [
    "MIN_FREQ = 1 ## Minimum token frequency to include in the vocab\n",
    "\n",
    "### Generate the Vocabulary\n",
    "tok_to_idx = {}\n",
    "idx_to_tok =  list(sorted(set(['<unk>'] + [tok for tok,count in token_counts.items() if count >= MIN_FREQ])))\n",
    "tok_to_idx = {tok: idx for idx, tok in enumerate(idx_to_tok)}\n",
    "\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(idx_to_tok)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def sliding_window(seq, window_size, overlap):\n",
    "    for i in range(0, len(seq) - window_size, window_size - overlap):\n",
    "        yield [ tok_to_idx[tok] if tok in tok_to_idx else tok_to_idx['<unk>'] for tok in seq[i:i + window_size] ]\n",
    "\n",
    "\n",
    "## Generate dataset \n",
    "def generate_data(text, seq_length, overlap):\n",
    "    num_tokens = len(char_tokenized_text)\n",
    "    return jnp.array([ seq for seq in sliding_window(char_tokenized_text, seq_length, overlap)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Validate-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3550, 63) (3550, 63)\n",
      "(2840, 63) (2840, 63)\n",
      "(355, 63) (355, 63)\n",
      "(355, 63) (355, 63)\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 64\n",
    "WINDOW_OVERLAP = 10 \n",
    "\n",
    "data = generate_data(text, SEQUENCE_LENGTH, WINDOW_OVERLAP)\n",
    "X = data[:,:-1]\n",
    "Y = data[:,1:]\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "train_idxs = list(range(int(0.8*X.shape[0])))\n",
    "valid_idxs = list(range(int(0.8*X.shape[0]), int(0.9*X.shape[0])))\n",
    "test_idxs = list(range(int(0.9*X.shape[0]), X.shape[0]))\n",
    "X_train = X[train_idxs,:]\n",
    "Y_train = Y[train_idxs,:]\n",
    "print(X_train.shape, Y_train.shape)\n",
    "X_valid = X[valid_idxs,:]\n",
    "Y_valid = Y[valid_idxs,:]\n",
    "print(X_valid.shape, Y_valid.shape)\n",
    "X_test = X[test_idxs,:]\n",
    "Y_test = Y[test_idxs,:]\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "assert(X_train.shape[0] + X_valid.shape[0] + X_test.shape[0] == X.shape[0])\n",
    "assert(all(X_valid[i,:].shape[0] == SEQUENCE_LENGTH-1 for i in range(X_valid.shape[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Basic GRU Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Completed epoch=0, train loss=6.4597, valid perplexity=1.1092, elapsed=28.2568s\n",
      "INFO:__main__:Completed epoch=20, train loss=2.9397, valid perplexity=1.0963, elapsed=66.1678s\n",
      "INFO:__main__:Completed epoch=40, train loss=2.5822, valid perplexity=1.0838, elapsed=103.3859s\n",
      "INFO:__main__:Completed epoch=60, train loss=2.3959, valid perplexity=1.0777, elapsed=141.1152s\n",
      "INFO:__main__:Completed epoch=80, train loss=2.2776, valid perplexity=1.0739, elapsed=179.3270s\n",
      "INFO:__main__:Completed epoch=100, train loss=2.1850, valid perplexity=1.0707, elapsed=218.1402s\n",
      "INFO:__main__:Completed epoch=120, train loss=2.1196, valid perplexity=1.0687, elapsed=256.2913s\n",
      "INFO:__main__:Completed epoch=140, train loss=2.0657, valid perplexity=1.0672, elapsed=294.6254s\n",
      "INFO:__main__:Completed epoch=160, train loss=2.0219, valid perplexity=1.0659, elapsed=334.0070s\n",
      "INFO:__main__:Completed epoch=180, train loss=1.9789, valid perplexity=1.0649, elapsed=372.3579s\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from blinker import signal\n",
    "\n",
    "import jax\n",
    "\n",
    "from xjax.models import gru\n",
    "from xjax.signals import train_epoch_started, train_epoch_completed\n",
    "\n",
    "rng = jax.random.key(42)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "HIDDEN_SIZE=128\n",
    "EPOCHS=200\n",
    "BATCH_SIZE=128\n",
    "LEARNING_RATE = 2*10**(-3)\n",
    "MAX_GRAD = 1\n",
    "\n",
    "params, baseline_model = gru.gru(rng, vocab_size=vocab_size, hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "@train_epoch_completed.connect_via(baseline_model)\n",
    "def collect_events(_, *, epoch, train_loss, valid_perplexity, elapsed, **__):\n",
    "    logger.info(f\"Completed epoch={epoch}, train loss={train_loss:0.4f}, valid perplexity={valid_perplexity:0.4f}, elapsed={elapsed:0.4f}s\")\n",
    "\n",
    "\n",
    "# I train a GRU model on the data \n",
    "baseline_params = gru.train(baseline_model, rng=rng, params=params, \n",
    "                                            X_train=X_train, Y_train=Y_train, \n",
    "                                            X_valid=X_valid, Y_valid=Y_valid, \n",
    "                                            vocab_size=vocab_size, \n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            num_epochs=EPOCHS, \n",
    "                                            learning_rate=LEARNING_RATE,\n",
    "                                            max_grad=MAX_GRAD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Basic GRU + Basic Dot-Product Self-Attention Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Completed epoch=0, train loss=5.5176, valid_perplexity=1.1142, elapsed=97.8642s\n",
      "INFO:__main__:Completed epoch=20, train loss=2.8486, valid_perplexity=1.0929, elapsed=152.2923s\n",
      "INFO:__main__:Completed epoch=40, train loss=2.4838, valid_perplexity=1.0805, elapsed=206.8317s\n",
      "INFO:__main__:Completed epoch=60, train loss=2.2775, valid_perplexity=1.0736, elapsed=260.7903s\n",
      "INFO:__main__:Completed epoch=80, train loss=2.1182, valid_perplexity=1.0687, elapsed=314.6506s\n",
      "INFO:__main__:Completed epoch=100, train loss=2.0077, valid_perplexity=1.0653, elapsed=369.5693s\n",
      "INFO:__main__:Completed epoch=120, train loss=1.9093, valid_perplexity=1.0627, elapsed=423.3096s\n",
      "INFO:__main__:Completed epoch=140, train loss=1.8364, valid_perplexity=1.0609, elapsed=477.3755s\n",
      "INFO:__main__:Completed epoch=160, train loss=1.7748, valid_perplexity=1.0595, elapsed=531.0542s\n",
      "INFO:__main__:Completed epoch=180, train loss=1.7265, valid_perplexity=1.0585, elapsed=584.8786s\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from blinker import signal\n",
    "\n",
    "import jax\n",
    "from xjax.models import gru_attn\n",
    "from xjax.signals import train_epoch_started, train_epoch_completed\n",
    "\n",
    "rng = jax.random.key(42)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "HIDDEN_SIZE=128\n",
    "EPOCHS=200\n",
    "BATCH_SIZE=128\n",
    "LEARNING_RATE = 2*10**(-3)\n",
    "MAX_GRAD = 1\n",
    "\n",
    "params, candidate_model = gru_attn.gru(rng, vocab_size=vocab_size, hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "@train_epoch_completed.connect_via(candidate_model)\n",
    "def collect_events(_, *, epoch, train_loss, valid_perplexity, elapsed, **__):\n",
    "    logger.info(f\"Completed epoch={epoch}, train loss={train_loss:0.4f}, valid_perplexity={valid_perplexity:0.4f}, elapsed={elapsed:0.4f}s\")\n",
    "\n",
    "\n",
    "# I train a GRU model on the data \n",
    "candidate_params = gru_attn.train(candidate_model, rng=rng, params=params, \n",
    "                                            X_train=X_train, Y_train=Y_train, \n",
    "                                            X_valid=X_valid, Y_valid=Y_valid, \n",
    "                                            vocab_size=vocab_size, \n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            num_epochs=EPOCHS, \n",
    "                                            learning_rate=LEARNING_RATE,\n",
    "                                            max_grad=MAX_GRAD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xjax\n",
    "\n",
    "# I generate sequences from a prefix\n",
    "prefix_str = \"the time\"\n",
    "prefix = [ tok_to_idx[i] for i in list(prefix_str)]\n",
    "baseline_results = []\n",
    "candidate_results = []\n",
    "for i in range(20):\n",
    "    rng, sub_rng = jax.random.split(rng)\n",
    "    y_baseline = gru.generate(rng=sub_rng, prefix=prefix, params=baseline_params, \n",
    "                 hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, max_len=30) \n",
    "    baseline_results.append(\"\".join([idx_to_tok[i] for i in y_baseline]))\n",
    "    y_candidate = gru_attn.generate(rng=sub_rng, prefix=prefix, params=candidate_params, \n",
    "                 hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, max_len=30) \n",
    "    candidate_results.append(\"\".join([idx_to_tok[i] for i in y_candidate]))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the time frishor the ire pranation nen        the time framblatures hery and ran nea\n",
      "the time dose stousters the save had t        the time theils of the traps and had t\n",
      "the time in one was and gare wor the s        the time into the and was the opeture \n",
      "the time saarn of the dision but some         the time satch of the din of rurding b\n",
      "the time fald door the than peed treen        the time fall door breex jumped other \n",
      "the time doon of the concely thing dow        the time enour five this traveller dow\n",
      "the time til of thet remest the staren        the time time through the mind the wen\n",
      "the time that of mingevery out my deme        the time traveller ngenecy out of the \n",
      "the timem thave chace a preeltering fa        the timemenalion pace and belooning fa\n",
      "the time with upea soor it whit that p        the time with up t his buind i legted \n",
      "the time erchige ertain the tran the m        the time each seeves i mosch can and w\n",
      "the time thinuly ground a but was dour        the time tried i had in a back and bee\n",
      "the time we all but a misk were un fev        the time were see dark i knwen must fo\n",
      "the time i satation ut must kext blofe        the time i dark but under from the mac\n",
      "the time to fearely to explace not the        the time to fear me to extance not the\n",
      "the time tiage they not for the sussed        the time trage of the time resicily be\n",
      "the time pemsear and ale i then their         the time peosecrial was of the brown t\n",
      "the time linted this the to leaficited        the time lint my all the took afocta c\n",
      "the time gimdd that laned the love tha        the time greding and lay be have methe\n",
      "the time the entol the prly wiok went         the time the stool they was wior were \n"
     ]
    }
   ],
   "source": [
    "for b,c in zip(baseline_results, candidate_results):\n",
    "    print(b,\"      \", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Perplexity on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Perplexity: 1.0761\n",
      "Candidate Perplexity: 1.0727\n"
     ]
    }
   ],
   "source": [
    "baseline_perplexity = xjax.models.gru.perplexity(baseline_model, baseline_params, vocab_size, X_test, Y_test)\n",
    "candidate_perplexity = xjax.models.gru_attn.perplexity(candidate_model, candidate_params, vocab_size, X_test, Y_test)\n",
    "print(f\"Baseline Perplexity: {baseline_perplexity:0.4f}\\nCandidate Perplexity: {candidate_perplexity:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
