{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Performance of Gated Recurrent Units With and Without Attention\n",
    "...by learning to predict the text of H.G. Wells' The Time Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = open('../datasets/timemachine.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's tokenize the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikram/dev/xjax/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Remove punctuation and convert to lowercase\n",
    "text = raw_text.lower()\n",
    "text = re.sub(r'[^a-z]+', ' ', text)\n",
    "\n",
    "# Tokenize into characters\n",
    "char_tokenized_text = list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 27\n",
      "  35850\n",
      "e 19667\n",
      "t 15040\n",
      "a 12700\n",
      "i 11254\n",
      "o 11082\n",
      "n 10943\n",
      "s 9242\n",
      "r 8833\n",
      "h 8786\n"
     ]
    }
   ],
   "source": [
    "# Most common tokens\n",
    "from collections import Counter\n",
    "# Flatten the list of tokens\n",
    "# Count the tokens\n",
    "token_counts = Counter(char_tokenized_text)\n",
    "print(f\"Total tokens: {len(token_counts)}\")\n",
    "# Most common tokens\n",
    "most_common_tokens = token_counts.most_common(10)\n",
    "for token, count in most_common_tokens:\n",
    "    print(f\"{token} {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 28\n"
     ]
    }
   ],
   "source": [
    "MIN_FREQ = 1 ## Minimum token frequency to include in the vocab\n",
    "\n",
    "### Generate the Vocabulary\n",
    "tok_to_idx = {}\n",
    "idx_to_tok =  list(sorted(set(['<unk>'] + [tok for tok,count in token_counts.items() if count >= MIN_FREQ])))\n",
    "tok_to_idx = {tok: idx for idx, tok in enumerate(idx_to_tok)}\n",
    "\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(idx_to_tok)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def sliding_window(seq, window_size, overlap):\n",
    "    for i in range(0, len(seq) - window_size, window_size - overlap):\n",
    "        yield [ tok_to_idx[tok] if tok in tok_to_idx else tok_to_idx['<unk>'] for tok in seq[i:i + window_size] ]\n",
    "\n",
    "\n",
    "## Generate dataset \n",
    "def generate_data(text, seq_length, overlap):\n",
    "    num_tokens = len(char_tokenized_text)\n",
    "    return jnp.array([ seq for seq in sliding_window(char_tokenized_text, seq_length, overlap)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Validate-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3550, 63) (3550, 63)\n",
      "(2840, 63) (2840, 63)\n",
      "(355, 63) (355, 63)\n",
      "(355, 63) (355, 63)\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 64\n",
    "WINDOW_OVERLAP = 10 \n",
    "\n",
    "data = generate_data(text, SEQUENCE_LENGTH, WINDOW_OVERLAP)\n",
    "X = data[:,:-1]\n",
    "Y = data[:,1:]\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "train_idxs = list(range(int(0.8*X.shape[0])))\n",
    "valid_idxs = list(range(int(0.8*X.shape[0]), int(0.9*X.shape[0])))\n",
    "test_idxs = list(range(int(0.9*X.shape[0]), X.shape[0]))\n",
    "X_train = X[train_idxs,:]\n",
    "Y_train = Y[train_idxs,:]\n",
    "print(X_train.shape, Y_train.shape)\n",
    "X_valid = X[valid_idxs,:]\n",
    "Y_valid = Y[valid_idxs,:]\n",
    "print(X_valid.shape, Y_valid.shape)\n",
    "X_test = X[test_idxs,:]\n",
    "Y_test = Y[test_idxs,:]\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "assert(X_train.shape[0] + X_valid.shape[0] + X_test.shape[0] == X.shape[0])\n",
    "assert(all(X_valid[i,:].shape[0] == SEQUENCE_LENGTH-1 for i in range(X_valid.shape[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Basic GRU Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Completed epoch=0, train loss=6.4597, valid perplexity=1.1092, elapsed=28.2568s\n",
      "INFO:__main__:Completed epoch=20, train loss=2.9397, valid perplexity=1.0963, elapsed=66.1678s\n",
      "INFO:__main__:Completed epoch=40, train loss=2.5822, valid perplexity=1.0838, elapsed=103.3859s\n",
      "INFO:__main__:Completed epoch=60, train loss=2.3959, valid perplexity=1.0777, elapsed=141.1152s\n",
      "INFO:__main__:Completed epoch=80, train loss=2.2776, valid perplexity=1.0739, elapsed=179.3270s\n",
      "INFO:__main__:Completed epoch=100, train loss=2.1850, valid perplexity=1.0707, elapsed=218.1402s\n",
      "INFO:__main__:Completed epoch=120, train loss=2.1196, valid perplexity=1.0687, elapsed=256.2913s\n",
      "INFO:__main__:Completed epoch=140, train loss=2.0657, valid perplexity=1.0672, elapsed=294.6254s\n",
      "INFO:__main__:Completed epoch=160, train loss=2.0219, valid perplexity=1.0659, elapsed=334.0070s\n",
      "INFO:__main__:Completed epoch=180, train loss=1.9789, valid perplexity=1.0649, elapsed=372.3579s\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from blinker import signal\n",
    "\n",
    "import jax\n",
    "\n",
    "from xjax.models import gru\n",
    "from xjax.signals import train_epoch_started, train_epoch_completed\n",
    "\n",
    "rng = jax.random.key(42)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "HIDDEN_SIZE=128\n",
    "EPOCHS=200\n",
    "BATCH_SIZE=128\n",
    "LEARNING_RATE = 2*10**(-3)\n",
    "MAX_GRAD = 1\n",
    "\n",
    "params, baseline_model = gru.gru(rng, vocab_size=vocab_size, hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "@train_epoch_completed.connect_via(baseline_model)\n",
    "def collect_events(_, *, epoch, train_loss, valid_perplexity, elapsed, **__):\n",
    "    logger.info(f\"Completed epoch={epoch}, train loss={train_loss:0.4f}, valid perplexity={valid_perplexity:0.4f}, elapsed={elapsed:0.4f}s\")\n",
    "\n",
    "\n",
    "# I train a GRU model on the data \n",
    "baseline_params = gru.train(baseline_model, rng=rng, params=params, \n",
    "                                            X_train=X_train, Y_train=Y_train, \n",
    "                                            X_valid=X_valid, Y_valid=Y_valid, \n",
    "                                            vocab_size=vocab_size, \n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            num_epochs=EPOCHS, \n",
    "                                            learning_rate=LEARNING_RATE,\n",
    "                                            max_grad=MAX_GRAD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Basic GRU + Basic Dot-Product Self-Attention Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Completed epoch=0, train loss=5.5176, valid_perplexity=1.1142, elapsed=97.8642s\n",
      "INFO:__main__:Completed epoch=20, train loss=2.8486, valid_perplexity=1.0929, elapsed=152.2923s\n",
      "INFO:__main__:Completed epoch=40, train loss=2.4838, valid_perplexity=1.0805, elapsed=206.8317s\n",
      "INFO:__main__:Completed epoch=60, train loss=2.2775, valid_perplexity=1.0736, elapsed=260.7903s\n",
      "INFO:__main__:Completed epoch=80, train loss=2.1182, valid_perplexity=1.0687, elapsed=314.6506s\n",
      "INFO:__main__:Completed epoch=100, train loss=2.0077, valid_perplexity=1.0653, elapsed=369.5693s\n",
      "INFO:__main__:Completed epoch=120, train loss=1.9093, valid_perplexity=1.0627, elapsed=423.3096s\n",
      "INFO:__main__:Completed epoch=140, train loss=1.8364, valid_perplexity=1.0609, elapsed=477.3755s\n",
      "INFO:__main__:Completed epoch=160, train loss=1.7748, valid_perplexity=1.0595, elapsed=531.0542s\n",
      "INFO:__main__:Completed epoch=180, train loss=1.7265, valid_perplexity=1.0585, elapsed=584.8786s\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from blinker import signal\n",
    "\n",
    "import jax\n",
    "from xjax.models import gru_attn\n",
    "from xjax.signals import train_epoch_started, train_epoch_completed\n",
    "\n",
    "rng = jax.random.key(42)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "HIDDEN_SIZE=128\n",
    "EPOCHS=200\n",
    "BATCH_SIZE=128\n",
    "LEARNING_RATE = 2*10**(-3)\n",
    "MAX_GRAD = 1\n",
    "\n",
    "params, candidate_model = gru_attn.gru(rng, vocab_size=vocab_size, hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "@train_epoch_completed.connect_via(candidate_model)\n",
    "def collect_events(_, *, epoch, train_loss, valid_perplexity, elapsed, **__):\n",
    "    logger.info(f\"Completed epoch={epoch}, train loss={train_loss:0.4f}, valid_perplexity={valid_perplexity:0.4f}, elapsed={elapsed:0.4f}s\")\n",
    "\n",
    "\n",
    "# I train a GRU model on the data \n",
    "candidate_params = gru_attn.train(candidate_model, rng=rng, params=params, \n",
    "                                            X_train=X_train, Y_train=Y_train, \n",
    "                                            X_valid=X_valid, Y_valid=Y_valid, \n",
    "                                            vocab_size=vocab_size, \n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            num_epochs=EPOCHS, \n",
    "                                            learning_rate=LEARNING_RATE,\n",
    "                                            max_grad=MAX_GRAD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xjax\n",
    "\n",
    "# I generate sequences from a prefix\n",
    "prefix_str = \"the time\"\n",
    "prefix = [ tok_to_idx[i] for i in list(prefix_str)]\n",
    "baseline_results = []\n",
    "candidate_results = []\n",
    "for i in range(20):\n",
    "    rng, sub_rng = jax.random.split(rng)\n",
    "    y_baseline = gru.generate(rng=sub_rng, prefix=prefix, params=baseline_params, \n",
    "                 hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, max_len=30) \n",
    "    baseline_results.append(\"\".join([idx_to_tok[i] for i in y_baseline]))\n",
    "    y_candidate = gru_attn.generate(rng=sub_rng, prefix=prefix, params=candidate_params, \n",
    "                 hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, max_len=30) \n",
    "    candidate_results.append(\"\".join([idx_to_tok[i] for i in y_candidate]))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the time was in it had had sale jight         the time was smart in the morlocking t\n",
      "the time in the fam i wat the time thi        the time in the far in the psity all i\n",
      "the time a ffike a mizid preet very di        the time and i singer i had survery di\n",
      "the time and at with in an the dable s        the time at the with mire was earth he\n",
      "the time the lices and were raallisuse        the time there you and were raw wiruse\n",
      "the time the lagate the blich of i fou        the time refoll as this alf had no in \n",
      "the time sove the way enound of tole i        the time to deem come in my eople leve\n",
      "the time were all it logk at as in the        the time were all it loggeesay mind th\n",
      "the time some his all the dapreess rem        the time some of my everys becouss ope\n",
      "the time to rer peplly fil the expelt         the time traver there of my coneas of \n",
      "the time and ruint discume whike it th        the time at the strain the fige with a\n",
      "the time that that upon to fechey that        the time that some upon the eveny that\n",
      "the time wiwen in sunkent and rime the        the time traveller under nature that s\n",
      "the time time my he slap now and undet        the time traveller frlachine and under\n",
      "the time a of we time stoom of seement        the time as i was in the sine is again\n",
      "the time the had the tide hose rean to        the time trace but of a seeps of from \n",
      "the time two sais crimight i soped the        the time two be shiveright i soperelat\n",
      "the time biested and had the relelion         the time minst of the a strange sleep \n",
      "the time incaiking contlest of the win        the time incrigat cook whizel all for \n",
      "the time that more upalound was of it         the time that more upand become one va\n"
     ]
    }
   ],
   "source": [
    "for b,c in zip(baseline_results, candidate_results):\n",
    "    print(b,\"      \", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Perplexity on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Perplexity: 1.0761\n",
      "Candidate Perplexity: 1.0727\n"
     ]
    }
   ],
   "source": [
    "baseline_perplexity = xjax.models.gru.perplexity(baseline_model, baseline_params, vocab_size, X_test, Y_test)\n",
    "candidate_perplexity = xjax.models.gru_attn.perplexity(candidate_model, candidate_params, vocab_size, X_test, Y_test)\n",
    "print(f\"Baseline Perplexity: {baseline_perplexity:0.4f}\\nCandidate Perplexity: {candidate_perplexity:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
